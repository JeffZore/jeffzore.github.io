---
layout: post
title: "Evolution of Aegis: Fault Diagnosis for AI Model Training Service in Production"
date:   2025-09-16
tags: [PapperReading,NSDI25]
comments: true
author: Jeffrey
---

## 摘要
阿里巴巴  nsdi25
尽管传统云计算中的诊断系统取得了成功，但由于传统云计算和模型训练在计算范式上存在差异，这些系统并不适用于精确查找人工智能模型训练云场景中的故障。
Aegis，一款专门为人工智能模型训练服务设计的故障诊断系统。Aegis第一阶段以易于部署为首要原则，从改进现有的通用诊断系统入手。
Aegis第二阶段定制了集体通信库，以便在运行时进行精确的故障定位，同时无需修改客户代码。
Aegis还赋予了处理性能下降问题和交付前故障检查的能力。
Aegis已在生产训练云服务中部署了一年。Aegis将诊断所浪费的闲置时间减少了97%以上，训练任务重启次数减少了84%，性能下降情况减少了71%。

<!-- more -->

# 背景

Reliability is important

**Why maintaining the reliability is challenging?**

传统方法：

The diagnosis systems used in traditional cloud computing \[23–26, 28, 29, 31, 32, 34, 35,37–43, 46–48, 50–55, 57–59, 61, 63, 64\], in principle, localize the root causes by tracing back the source-destination path through the sequence of system component calls such as the 5-tuples or devices of given faults;

五元组和序列定位来进行溯源定位   -\> 如何理解这部分的差异

现有集群训练的任务几乎都是单点故障 ，传统方法无法实现定位和根因分析

The state-of-the-art efforts limitation.

最先进的通用诊断系统 \[23–26,28,29,31,32,34,35,37–43,46–48,50–55,57–59,61–64\]朝着处理网络之外的故障迈出了一步。其中一些系统还进一步考虑了不同设备之间的相关性，以提高分析准确性 \[29,30,33,35,44\]。

然而，由于缺乏对诸如集体通信等模型训练特定特征的考虑，它们无法涵盖模型训练场景中的许多故障。

例如，在大型语言模型（LLM）训练中，每次集体通信都会使数十甚至数百个 GPU同步运行。简单地将这些相关性分析方法扩展到如此大规模，会导致需要在许多节点之间进行比较。根据我们的经验，这会导致诊断延迟不可接受地增加，并造成准确性损失。

因此，我们转而研究是否有专门为模型训练场景设计的诊断系统。有两种具有代表性的诊断解决方案是为模型训练集群构建并在生产环境中大规模部署的（即微软部署的 SuperBench \[60\] 和字节跳动部署的 MegaScale \[36\]）。遗憾的是，如图5所示，由于以下原因，这两种方案都无法完全满足模型训练云场景的需求。

SuperBench提供了一套全面的基准测试套件，用于在集群部署前进行灰度故障检查。虽然它很有用，但很难定位模型训练运行时出现的故障根源。

Time-consuming offline diagnosis is insufficient.  离线耗时无法接受 。superbench的做法需要进行长时间的测试  一次测试的时间也会非常长 单次测试要10分钟 整体要持续数个小时。延长了每次训练失败后的等待时间。

MegaScale 通过监控客户模型中“关键代码段”的 CUDA事件来识别故障。这仅适用于模型设计者和模型训练服务提供商属于同一方的场景；否则，没有模型设计者（即客户）愿意允许训练服务提供商监控或修改他们的代码。这样的假设对于公共模型训练服务提供商来说是不切实际的。

Deep coupling with customers’ code is inappropriate. 侵入用户代码无法接受 需要用户配合来识别关键的代码片段

监控CUDA事件需要在调用这些CUDA事件的同一线程中精确初始化一个定制的监控模块。这意味着这种初始化只能在客户的模型代码中显式调用。部署一个强制修改客户模型代码的新工具是很困难的。我们的销售人员与客户之间的协商主要阻碍了整个部署过程。

**Aegis**

key goal: diagnosing root causes of failures in service runtime without modifying customer code.

以实现我们的关键目标：在不修改客户代码的情况下诊断服务运行时故障的根本原因。

原因如下：

（1）我们需要覆盖整个生命周期内的故障，而不仅仅是集群部署阶段；

（2）我们的诊断系统应该对各类客户具有通用性和透明度。

本文分享了我们在Aegis的动机（§2）、设计（§3）和演进（§4）方面的经验。

在Aegis的第一阶段（第4.1节），我们用训练输出日志和适用于大规模模型训练场景的诊断流程对现有的诊断系统进行了改进。这一改进使我们能够在大多数故障情况下，将整个任务级别的故障范围缩小到具体的出错设备。对于更复杂的情况，我们采用离线诊断作为兜底措施。

从NCCL层面做代码侵入的原因：

(1) in mainstream training frameworks (e.g., Megatron \[49\] and DeepSpeed \[8\]), CCL is integrated as an independent plugin. Customizing CCL would not introduce any code modification in customers’ models or training frameworks.

NCCL是一个在主流框架中使用的集合通信库

(2) CCL “**sits at the boundary**” of computation and communication, making it a perfect place for generating ideal failure diagnosis information. Through defining appropriate CCL metrics and constructing the corresponding runtime diagnosis system, we improve the runtime diagnosis ratio from 77% to close 100%.

集合通信库是一个计算和通信的边界

除故障诊断外，显著的性能下降是另一个关键问题。根据我们的经验，我们进一步改进了基本指标关联诊断和增强的过程感知诊断方法，以找出性能下降的根本原因（§5）。除了解决训练过程中的问题外，我们注意到超过73%的任务在初始化阶段失败。这一现象表明，在模型训练任务开始之前，集群中就已经存在错误。为了尽量减少客户不必要的重试，我们部署了交付前检查（CBD）机制，在将主机交付给客户之前进行必要的检查（§6）。

我们展示了在Aegis演进过程中，我们的训练集群的诊断效率（§7）。出于保密原因，我们以内部大型语言模型（LLM）训练项目的统计数据作为代表。在过去的16个月里，我们项目的训练规模增长了40多倍。令人印象深刻的是，随着Aegis第一阶段的部署，由故障诊断导致的GPU空闲时间减少了71%。随着Aegis第二阶段的部署，几乎所有模型训练任务的故障都能在运行时被诊断出来。GPU空闲时间进一步减少了91%。在处理性能下降问题时，随着关联诊断和增强的过程感知诊断的逐步部署，性能下降程度降低了71%。随着CBD的部署，训练任务重启次数减少了84%。

# 挑战&问题

## Challenges Introduced by Model Training

**Higher hardware failure ratio**

1)GPU性能越高 故障率越大

<img src=":/6563fb15125d436a9d11e280be4077fa" alt="b3056ccbfbd8e44aba089af8f232f26d.png" width="539" height="296" class="jop-noMdConv">

2)Complex intra-host network topology.

3)More link failures in large-scale model training cluster.光纤模块故障率更高

*However, optic modules and fiber have a higher failure ratio \[18\]*

Ten Reasons to Use Passive DAC Cables in Your Data Center. https://vitextech.com/ten-reasons-to-use-passive-dac-cables-in-your-data-center/, 2024.  分析DAC和AOC 铜缆之间的差异

**Lacking direct root cause indicators**

1）单点故障导致集群所有主机的任务失败 ，不像传统任务的失败指在几个节点异常

2）崩溃时才能看到异常，多由于网络超时触发->网络是分析问题的一个出发点 但不一定是根因

## Why Our Existing Systems Do Not Help

three diagnosis tools used in our data center networks for general cloud computing.

**Network monitoring and analysis.**

网卡（NIC）和交换机都会持续生成记录其运行状态的日志，特别是警告和错误消息。主机和交换机中部署了一系列基于统计的监控器（例如，每个端口的接收/发送吞吐量、乱序数据包数量、显式拥塞通知（ECN）标记数量）。收集和分析来自网卡和交换机的日志及统计数据。通过匹配特定的网络故障模式（例如，日志中显示的严重错误和异常统计数据），它可以自动识别故障节点并将其隔离。

**RDMA Pingmesh.**

类似tcp pingmesh 和  rdma ping mesh

**Inband network diagnosis.**

上述两个系统负责利用端到端信息来诊断训练集群中的故障设备。然而，在复杂的故障场景中，还需要逐跳统计信息，来找出故障的根源。我们构建了这样一个系统，它可以对特定数据包进行标记，并追踪它们在每个单独交换机之间的转发状态。

这部分没有说细节

**Limitations.**

1、Focusing on the network itself.

2、Focusing on single request/response.

传统的请求响应式诊断方式 侧重在单个连接的问题

大规模训练需要关联多个设备信息来诊断

# 解决方案&思路  -\> Aegis Overview

Aegis第一阶段，我们进一步引入训练日志中的错误信息，并构建了一个特定于训练的运行时诊断程序（§4.1.1）。

我们还为运行时诊断无法处理的棘手情况设计了一个全面的离线诊断后备方案（§4.1.2）。

在运行过程中，我们发现有相当数量的失败案例需要进行离线诊断，这导致GPU利用率不理想。因此，我们将Aegis发展到第二阶段，通过定制CCL来获取更多特定于训练的信息，以进行训练过程感知诊断（§4.2）。

在处理性能下降问题时，根据以往经验，我们设计了指标相关性诊断（§5.1）和增强型过程感知性能下降诊断（§5.2）。

为进一步提升用户体验，我们增加了一个新的“预上线”流程，在交付给客户之前进行高效的集群检查（§6）。

## Task Failure Diagnosis

### **Phase-1: Enhancing Existing Systems**

**Basic error diagnosis :**

通过收集实际集群中的各种日志消息 手动进行定位 并进行自动化的诊断分析

克服了两个困难：

**1、Not all reported errors are critical.  不是所有的报错都代表机器有问题**

eg:ECC错误有多种类型。只有双比特ECC错误（例如，XID48错误 \[21\]）和不可纠正的ECC错误（例如，XID94/95错误）是导致失败的根本原因。其他单比特ECC错误（例如，XID92错误）和可纠正的ECC错误（例如，XID63/64错误）表明HBM内存存在错误，但不会引发失败。

在任务崩溃时间附近，不同主机都会报告大量错误。如果将所有这些主机直接隔离，训练集群的整体利用率将受到极大损害。

--\> 核心是 即使将问题转化为定位机器 也会变成 定位根因机器   有些机器并不是引起问题的根本原因

解决办法：根据经验 寻找一定是关键错误的点 如：掉卡、 nvlink、 pcie lane drop

**2、Not all critical errors point to a clear location. 并非所有的严重错误都能明确指出错误位置。**

eg:crash of network connections (“connection reset by peer”).

构建了一个分布式错误列表来记录这些错误：DistError()。

报错处理算法：如果错误是一台机器的关键错误 立刻进行隔离并重启。如果是2台 同样一起隔离 多台会有资源损失 这里设定2

如果是涉及多台的机器  采用分析程序进行分析  确定这些问题的源头（没说分析源头的方法）

如果错误的分布没有明显规律 很可能是有系统性问题 。ConfigCheck() 和 NetDiag()来进行进一步诊断。ConfigCheck()维护了一份检查清单和相应的脚本，用于识别配置错误的各种原因。NetDiag()由 §2.2中介绍的现有数据中心网络（DCN）诊断系统构建而成。

如果上述所有的操作都无法定位问题，那么将隔离所有节点 离线诊断

Lesson: Exhausting host-side critical failures first is the most efficient way to diagnose. 

**Offline failure diagnosis**

在整个诊断流程结束后，一些问题仍无法凭借现有的运行时信息直接诊断出来。为了定位这些故障的根本原因，设计了一种离线故障定位机制。

该机制会对当前训练任务中使用的所有可疑主机进行隔离和诊断。

与SuperBench \[60\]占用整个集群进行数小时测试不同，我们的离线系统会对目标主机**并行诊断故障**。

**1、Parallelized offline failure localization.**

先进行单机压测  对CPU, GPU, PCIe, and NVLink 进行重点压力测试

在进行多机，多机类似superbench 同时会增加模型种类 如多模态 MoE

The basic idea is that the endto-end training failure is triggered by a specific combination of computing and communication in the customer’s model.  用户特殊的计算通信模式会触发一些问题  --> 文中好像没有例子 

**2、Topology-aware parallel localization. 拓扑感知的并行诊断**

basic idea：如果是网络故障  并行任务使用了相同的问题链路 两个任务都会受到影响 

如果是主机故障 任务之间可能拥塞导致测的有问题

因此按照pod 和tor关系 分别进行并行测试  二分查找到没问题的机器恢复线上服务  汇聚到系统tor的任务可以任意分割

**3、Transforming the missing piece into a new clue. 将缺失的线索转化为新的线索**

上述两个方法都没有经过core层 所以不能诊断发生在core层的问题

实例：有一个占用1500个GPU的训练任务失败了。在离线故障诊断中，成功找到了一个参考模型来复现该故障。然而，当我们采用并行诊断时，在任何主机子集中都无法复现该故障。经过多次复现和进一步分析，我们从这个未解决的问题中获得了启示，并得出结论：聚合交换机一定存在某些异常。通过后续针对交换机的诊断，根本原因明确了：有一台聚合交换机出现了静默丢包的情况。但为什么在线诊断中的NetDiag()没有检测到这个交换机错误呢？我们遇到了一种特殊的静默丢包情况，它只丢弃大于1KB的数据包。因此，由于所有探测包的大小都是64B，RDMA Pingmesh系统没有抛出任何错误。

基于此构建自动诊断测试  同时加强rdmapingmesh 覆盖更大规模的数据大小

### Phase-2: Procedure-aware Diagnosis

we upgrade our system to diagnose runtime failures using a procedure-aware approach.

限制：

High confidentiality. 高度保密  （感觉这个用词和后面内容不符  高度特异性？）

Minimal customer modifications. 低侵入

Low overhead 低开销

<img src=":/410deea440d8432b893ff0e10b9ac2db" alt="18bd8b0da58a2399ab8d64f33c733a63.png" width="546" height="236">

![e219dd530e7ebf8e0fe0f101bcc76da1.png](:/dc4d27831b754189940e6997f2d6a0d0)

Limitations. 

只能找到目标机器  不能分析根本原因。为了给所有客户提供一致的诊断能力，我们需要确保：(1)我们的诊断系统能够无缝部署到不同的镜像和模型训练任务中；(2)部署在不同环境中的解决方案应能提供相同的诊断信息，以实现一致的诊断性能。因此，我们需要根据所有CCL发布版本提供相应的定制版本。尽管在定制CCL时存在上述限制，但与其他替代方案（例如定制训练框架或修改客户的模型代码）相比，它仍然易于部署。

## Performance Degradation Diagnosis

 除了训练任务完全失败之外，一些设备异常可能不会导致整个训练崩溃，但会导致性能显著下降。这些异常也应及时诊断。由于性能下降发生时训练任务仍在运行，不能将离线诊断（OfflineDiag()）作为最终的备用解决方案。因此，我们设计了一个性能下降诊断系统。

1）Abnormal operating metrics。这些指标旨在直接表明某些组件正在异常条件下运行。例如，Retran指标表示每秒重传的数据包数量。在正常情况下，这个Retran指标应该始终为零。Retran指标值高表示网络出现故障。

2）Performance metrics。这些指标旨在反映特定组件的执行效率。一个组件的异常变化会导致整体性能下降。例如，Actual TensorFLOPS指标表示每秒完成的张量浮点计算次数。

我们根据生产环境中的操作情况选择了20多个指标，包括主机指标（如CPU利用率、GPU利用率、GPU温度和PCIe利用率）和网络指标（如带宽利用率、重传次数、交换机端口队列长度和显式拥塞通知计数）。由于保密政策，我们无法公布所使用的所有指标的详细列表。

这些指标的异常意味着性能的下降 （然而并没有公布细节）

但是 只使用静态阈值，无法应用到所有的故障场景

**Cross-host correlating diagnosis.**

不同主机对同一指标的监测结果在不同训练迭代中应遵循相同的变化模式。

基本思路是，一小部分节点会导致整体性能下降，并且性能下降通常伴随着某些指标的异常变化，这些变化可以指示根本原因。

因此，我们为不同指标设计了一个Z-Score \[22\]异常值分析器。

对于每个选定的指标，离群值分析器会计算时间段 T内的平均值 λ 和标准差 δ。如果单个主机的指标值在持续的时间段 T内高于 λ +2δ，则该主机被定义为离群值。在生产环境中，T被设置为10分钟。实际上，异常节点通常会产生与其他节点明显不同的值，这使得 λ +2δ成为生产环境中一个简单且足够好的诊断阈值。

Actually, we have tried a series of other outlier analysis mechanisms (e.g., LOF \[27\], Isolation Forest \[3\] and DBSCAN \[7\]). They lead to similar precision and recall ratios. Considering that this entire process needs to run in a stream processing way, the calculation cannot be too complex. We finally choose this simple but efficient outlier analysis mechanism. This correlating diagnosis helps us troubleshoot various malfunctioning devices, including decelerating GPU/CPU/PCIe/NIC/link/switch, which covers a large proportion of failures in production.

Actually, we have tried a series of other outlier analysis mechanisms (e.g., LOF \[27\], Isolation Forest \[3\] and DBSCAN \[7\]). They lead to similar precision and recall ratios. Considering that this entire process needs to run in a stream processing way, the calculation cannot be too complex. We finally choose this simple but efficient outlier analysis mechanism. This correlating diagnosis helps us troubleshoot various malfunctioning devices, including decelerating GPU/CPU/PCIe/NIC/link/switch, which covers a large proportion of failures in production.Actually, we have tried a series of other outlier analysis mechanisms (e.g., LOF \[27\], Isolation Forest \[3\] and DBSCAN \[7\]). They lead to similar precision and recall ratios. Considering that this entire process needs to run in a stream processing way, the calculation cannot be too complex. We finally choose this simple but efficient outlier analysis mechanism. This correlating diagnosis helps us troubleshoot various malfunctioning devices, including decelerating GPU/CPU/PCIe/NIC/link/switch, which covers a large proportion of failures in production.

![91c94b17af91872dfdc78169a768724f.png](:/297d56c812db42f0929f5a7b4b0258cb)

观测到EC数值的波动 （与上面的丢包相一致）

limitation：单一指标并不能适配所有场景  因此还需要在多个指标里 寻找异常

**Enhancing Procedure-aware Diagnosis  基于流程感知的诊断**

Inspired by the design choice in §4.2, we choose to further customize CCL for more information helping degradation diagnosis. We further record the following statistics of each collective operator (Ci) for each GPU (Gj) in iteration Ik. 对集合通信 做一下统计

<img src=":/fa88d990697744269b20442a1f2bceea" alt="f20e4553f1f04782347d5b32c10fc465.png" width="568" height="141">

图9a展示了计算性能下降的情况，其中计算耗时意外过长是导致性能下降的原因。

由于每个集体操作的结束是同步的，我们可以利用通信时间来推断计算时间。

如果TDi, j,k < αTCi,k（实际中α =0.8），那么Gj就是计算性能下降的根本原因。

图9b展示了通信性能下降的情况，其中通信耗时过长是导致性能下降的原因。如果Ni, j,k > βNi,k（实际中β =1.5），则存在通信性能下降的问题。我们在此使用一个松弛阈值来抵御可能由临时网络拥塞引起的噪声。

基于这些信息，我们筛选出直接受到性能下降影响的GPU组G。G进一步用于确定哪个源或目标是此次通信性能下降的根本原因。此过程的原理与附录A中算法1的RootDiag()类似。

## Solving Problems Before Delivery

初始化阶段出现的问题

1) Frequent component updates. 

2)Post-usage failures. 主机上次使用出现问题 在交给用户时  启动就会发生问题 （逻辑不是很通啊 有问题还交付啥）

CBD  Check Before Delivery

交付前的测试要尽可能时间短  因此选择了具有代表性的测试  整个执行过程不超过10分钟 同时还有一个1分钟的轻量版

通过部署CBD，我们在最终交付前拦截了1 -2%有问题的主机。如果未能及时检测出来，这些有问题的主机将导致训练任务失败。考虑到这一显著的益处，我们已将CBD设为交付过程中的一个强制环节。

# 实验&结论

`关注性能的提升有多少 数字 与谁相比`

## Evaluation: Aegis in Production

<img src=":/f3b10d17dbc14132b855bc61c30b3c24" alt="f4a3c4a80e41d2c7ba05569ef6541dee.png" width="577" height="512">

如图11所示，折线记录了内部模型训练团队的训练任务规模，在过去16个月里增长了40多倍。

柱状图表示由于等待故障诊断，训练任务每月累计的闲置时间。在“宙斯盾一期”上线后，次月的闲置时间（训练集群中无任务运行的时长）减少了71%。考虑到我们在2023年9月甚至将训练规模扩大了一倍，这一结果令人印象深刻。

![7fa069401deb4dd49a7a10a8a92ab905.png](:/a6591b7180f64c3fbd3d37cfa7c43468)

Aegis第二阶段于2024年6月部署，直接使训练空闲时间节省了91%。这一改进主要源于更多的故障可以在不进行离线诊断的情况下得到解决。

图12展示了训练任务的重启计数器。2023年11月训练任务规模的增加也引发了更多的训练重启，以及初始化阶段的大量故障。因此，我们加快了CBD的开发，并于2023年12月将其投入在线服务。这使得下个月的重启计数器数值降低了44.8%。通过持续处理更多案例并优化检查清单，重启计数器最终降低了84.6%。随着CBD的全面部署和综合优化，它在最终交付前发现了约1 -2%的有问题主机。请注意，2024年8月重启次数有所增加，原因是我们的模型训练团队将任务从预训练切换到了微调，引入了有计划的实验和测试。这些错误的根本原因多种多样（例如，设备老化、配置错误、维修导致的布线错误、复杂的服务模式切换等）。此外，由于我们为多个租户的模型训练请求提供服务，并非所有任务都成熟且运行稳定。许多故障是由未优化的大型语言模型设计或对训练基础设施的错误使用导致的。

Runtime Failure Diagnosis

我们在图13中进一步回答了在运行时（而非离线）诊断出的故障案例数量。这一指标很重要，因为每次离线诊断都会对整体GPU利用率和用户体验造成极大损害。随着Aegis第二阶段的部署，运行时诊断比例逐渐趋近于100%。这意味着训练任务几乎可以在无人干预的情况下从所有类型的故障中自动恢复。

Handling Performance Degradation

统计结果如图14所示。宙斯盾（Aegis）性能降级诊断于2024年6月部署，该诊断显著消除了71%的性能降级问题。

## Experience and Lessons

光纤污染 主要原因是数据中心建筑基础设施的建设时间与服务器和网络布线的安装时间重叠。这种重叠导致光模块和光纤受到严重污染。图15展示了一个月内故障率的变化。随着新机器分批交付，由于受污染的链路，故障链路的数量迅速增加。随着清洁工作的推进，故障率逐渐下降。需要进行数十次深度清洁才能彻底解决这个污染问题。在发现这个问题后，我们对数据中心的建设和交付实施了更严格的准则，以防止未来出现类似问题。

We eventually pinpointed the cause: a bug in the congestion control implementation of NICs. The bug causes an issue where a small number of continuous ECN signals could cause the NIC to enter a preset maximum rate limit. This rate limit is set very low, resulting in significant communication slowdowns.

网卡由于拥塞 导致陷入长期的慢速状态

# 扩展阅读

we design a Z-Score \[22\] outlier analyzer for different metrics.